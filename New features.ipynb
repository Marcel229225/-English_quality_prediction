{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e59611d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt \n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextstat\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlexical_diversity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lex_div \u001b[38;5;28;01mas\u001b[39;00m ld\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt \n",
    "import spacy\n",
    "import textstat\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "df = pd.read_csv(\"training_set_rel3.csv\")\n",
    "#df\n",
    "def gunning_fog(text):\n",
    "    return textstat.gunning_fog(text)\n",
    "\n",
    "def ari(text):\n",
    "    return textstat.automated_readability_index(text)\n",
    "\n",
    "def smog_index(text):\n",
    "    return textstat.smog_index(text)\n",
    "\n",
    "def flesch_kincaid(text):\n",
    "    return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "def coleman_liau(text):\n",
    "    return textstat.coleman_liau_index(text)\n",
    "\n",
    "def dale_chall_readability(text):\n",
    "    return textstat.dale_chall_readability_score(text)\n",
    "\n",
    "def root_ttr(text):\n",
    "    flt = ld.flemmatize(text)\n",
    "    return ld.root_ttr(flt)\n",
    "\n",
    "def ttr(text):\n",
    "    flt = ld.flemmatize(text)\n",
    "    return ld.ttr(flt)\n",
    "\n",
    "def log_ttr(text):\n",
    "    flt = ld.flemmatize(text)\n",
    "    return ld.log_ttr(flt)\n",
    "\n",
    "def mass_ttr(text):\n",
    "    flt = ld.flemmatize(text)\n",
    "    return ld.maas_ttr(flt)\n",
    "\n",
    "def msttr(text):\n",
    "    flt = ld.flemmatize(text)\n",
    "    return ld.msttr(flt)\n",
    "\n",
    "def mtld(text):\n",
    "    flt = ld.flemmatize(text)\n",
    "    return ld.mtld(flt)\n",
    "\n",
    "txt = df['essay'][0]\n",
    "print(mass_ttr(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        00  000  00pm  046  101st  102  102nd   11   12   13  ...  œthe  \\\n",
      "0      0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "1      0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "2      0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "3      0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "4      0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "...    ...  ...   ...  ...    ...  ...    ...  ...  ...  ...  ...   ...   \n",
      "12973  0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "12974  0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "12975  0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "12976  0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "12977  0.0  0.0   0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "\n",
      "       œthen  œwas  œwellâ  œwhat  œwhatâ  œwhen  œwhy  œyou  œyouâ  \n",
      "0        0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "1        0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "2        0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "3        0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "4        0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "...      ...   ...     ...    ...     ...    ...   ...   ...    ...  \n",
      "12973    0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "12974    0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "12975    0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "12976    0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "12977    0.0   0.0     0.0    0.0     0.0    0.0   0.0   0.0    0.0  \n",
      "\n",
      "[12978 rows x 38237 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = (df['essay']).tolist()\n",
    "#sample\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array for easier handling\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "dfx = pd.DataFrame(data=tfidf_array, columns=feature_names)\n",
    "print(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f19c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ability', 'abilitys', 'abiliy', 'abiliyt', 'abillity', 'abillty',\n",
       "       'abilty', 'abl', 'ablaze', 'able'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dfx.columns)[90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<DT>?<JJ>*<NN>'>\n",
      "(S\n",
      "  Also/RB\n",
      "  computers/NNS\n",
      "  will/MD\n",
      "  benefit/VB\n",
      "  us/PRP\n",
      "  by/IN\n",
      "  helping/VBG\n",
      "  with/IN\n",
      "  jobs/NNS\n",
      "  as/IN\n",
      "  in/IN\n",
      "  planning/VBG\n",
      "  (NP a/DT house/NN)\n",
      "  (NP plan/NN)\n",
      "  and/CC\n",
      "  typing/VBG\n",
      "  a/DT\n",
      "  @/JJ\n",
      "  NUM1/NNP\n",
      "  (NP page/NN)\n",
      "  (NP report/NN)\n",
      "  for/IN\n",
      "  one/CD\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  jobs/NNS\n",
      "  in/IN\n",
      "  less/JJR\n",
      "  than/IN\n",
      "  writing/VBG\n",
      "  it/PRP\n",
      "  ./.)\n",
      "<bound method Tree.subtrees of Tree('S', [('Also', 'RB'), ('computers', 'NNS'), ('will', 'MD'), ('benefit', 'VB'), ('us', 'PRP'), ('by', 'IN'), ('helping', 'VBG'), ('with', 'IN'), ('jobs', 'NNS'), ('as', 'IN'), ('in', 'IN'), ('planning', 'VBG'), Tree('NP', [('a', 'DT'), ('house', 'NN')]), Tree('NP', [('plan', 'NN')]), ('and', 'CC'), ('typing', 'VBG'), ('a', 'DT'), ('@', 'JJ'), ('NUM1', 'NNP'), Tree('NP', [('page', 'NN')]), Tree('NP', [('report', 'NN')]), ('for', 'IN'), ('one', 'CD'), ('of', 'IN'), ('our', 'PRP$'), ('jobs', 'NNS'), ('in', 'IN'), ('less', 'JJR'), ('than', 'IN'), ('writing', 'VBG'), ('it', 'PRP'), ('.', '.')])>\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kylet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.parse import EarleyChartParser\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_sent_tree_len(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    #print(cp)\n",
    "    tree = cp.parse(pos_tags)\n",
    "    #print(tree)\n",
    "    #print(tree.subtrees)\n",
    "    tree_length = len(list(tree.subtrees()))\n",
    "\n",
    "    return tree_length\n",
    "\n",
    "def get_para_tree_len(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    tree_lengths = [get_sent_tree_len(sentence) for sentence in sentences]\n",
    "    return tree_lengths\n",
    "\n",
    "def mean_tree_len(text):\n",
    "    paragraph_example = text\n",
    "    lengths = get_para_tree_len(paragraph_example)\n",
    "\n",
    "    size = (len(lengths))\n",
    "    total = 0\n",
    "    #print(\"Lengths of Sentence Trees:\")\n",
    "    for i, length in enumerate(lengths, 1):\n",
    "    #    print((sent_tokenize(text))[i - 1])\n",
    "    #    print(f\"Sentence {i}: {length}\")\n",
    "        total += length\n",
    "    return (total/size)\n",
    "\n",
    "#print(mean_tree_len(df['essay'][1]))\n",
    "#print(get_sentence_tree_length(\"Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422f90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
